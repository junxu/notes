## CNN papers

1. ResNet(2015): [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)
2. Wide ResNet(2015): [Wide Residual Networks](https://arxiv.org/abs/1605.07146)
3. ResNeXt(2016): [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431)
4. DenseNet(2017): [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)
5. Xception(2016): [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)
6. MobileNets(2017): [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)
7. MobileNetV2(2018): MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)
8. ShuffleNet(2017): [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://arxiv.org/abs/1707.01083)
9. Inception-v4(2016): [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)
10. Inception-v2(2015): [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
11. Inception-v3(2015): [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)
12. NASnet(2017): (Learning Transferable Architectures for Scalable Image Recognition)(https://arxiv.org/abs/1707.07012)
13. SqueezeNet(2016): [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size](https://arxiv.org/abs/1602.07360)
14. SENet(2017): [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)


